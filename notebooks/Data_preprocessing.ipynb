{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "%pip install geopandas\n",
    "%pip install pyrosm\n",
    "%pip install gtfs_kit\n",
    "%pip install h3\n",
    "%pip install pyproj\n",
    "%pip install keplergl\n",
    "%pip install matplotlib\n",
    "%pip install esy-osmfilter\n",
    "%pip install rasterio\n",
    "%pip install joypy\n",
    "%pip install sklearn\n",
    "%pip install pyproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aquisition and preprocessing for new danish Transit Acessibility score\n",
    "\n",
    "In this notebook we go through the different steps taken in order to produce the raw data needed for generating the transit accesibility map for Denmark as visualized here https://potatotvnet.github.io/transit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project utilizes data from a variety of different sources. These are listed here for completeness\n",
    "* We downloaded the entire schedule of the public transit in Denmark for a typical week in march 2022 (20/03/22 to 26/03/22) in the standard GTFS data format using the Rejseplanen API https://help.rejseplanen.dk/hc/da/articles/214174465-Rejseplanens-API (danish only). This data range does not include any special holidays.\n",
    "* We used OpenStreetMap data for the entirety of Denmark as collected by Geofabrik.de (https://download.geofabrik.de/europe/denmark.html). The data is used for generating the walkable paths that connect people to public transit.\n",
    "* We downloaded the Global Human Settlement Layers (GHSL) covering all of Denmark (https://ghsl.jrc.ec.europa.eu/download.php?ds=smod). The layers are satelite images and were used to determine the rural and urban areas of Denmark. \n",
    "\n",
    "In terms of Python libraries that work on geospatial data we highligt the following tools\n",
    "* H3 was used to convert points, stops and paths from the WGS 84 coordinate reference system to a spatial index of hexagons to decrease the amount of space and computation needed in order to work with this much data\n",
    "* GTFS kit provides powerful functionality to convert data from the GTFS format to more Python standard formats like the geodataframe from geopandas.\n",
    "* Geopandas/pandas standard tools for data wrangling\n",
    "* esy-osmfilter Loading large pbf files (such as the danish OSM data) using libraries such as Pyrosm which converts the data to a GeoDataFrame is very memory intensive and should be avoided when working on a standard laptop. We use esy-osmfilter to filter the OSM data to include only what we want and save it to a geojson which we can work with in python.\n",
    "* Rasterio provides functionality to work with raster images such as satelite images. We use it for reading, rendering and projecting the GHSL data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import shapely as shp\n",
    "import gtfs_kit as gk\n",
    "from shapely.geometry import mapping, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GTFS feeds\n",
    "Initially we wanted to see if our methods would scale to include more than one transit ressource (ie multiple GTFS files) however we later found this to be out of scope for the project\n",
    "\n",
    "Specify the paths to the zip files here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_RES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read GTFS data from multiple GTFS files is possible\n",
    "paths = [\"../resources/rejseplanen.zip\"]\n",
    "feeds = []\n",
    "for path in paths:\n",
    "    feeds.append(gk.read_feed(path, dist_units='km'))\n",
    "\n",
    "dates = ['20220320', '20220321', '20220322', '20220323', '20220324', '20220325', '20220326']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map all stops to their corresponding H3 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map all stops to h3 for each feed (ie each GTFS file)\n",
    "stop_h3_maps = []\n",
    "for feed in feeds:\n",
    "    feed.stops['h3'] = feed.stops.apply(lambda row: h3.geo_to_h3(row['stop_lat'],row['stop_lon'],H3_RES), axis=1)\n",
    "    stop_h3_maps.append(pd.Series(feed.stops.h3.values,index=feed.stops.stop_id).to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write back data in geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "geojson = feed.routes_to_geojson(include_stops=True)\n",
    "with open(\"data/geojson.json\", \"w\") as outfile:\n",
    "    json.dump(geojson, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work covering the danish transit data\n",
    "\n",
    "Take the danish transit feed and turn the routes into a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "routes = gk.routes.geometrize_routes(feeds[0])\n",
    "routes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform geometries into arrays containing geometry points and write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_geo(geo):\n",
    "    if type(geo) == shp.geometry.LineString:\n",
    "        return [[g[0], g[1]] for g in list(geo.coords)]\n",
    "    else:\n",
    "        return [[g[0], g[1]] for g in list(geo.geoms[0].coords)]\n",
    "\n",
    "routes_filtered = routes\n",
    "routes_new = []\n",
    "\n",
    "for idx, route in routes_filtered.iterrows():\n",
    "    name = route[\"route_short_name\"]\n",
    "    category = route[\"route_type\"]\n",
    "    geo = route[\"geometry\"]\n",
    "    if type(geo) == shp.geometry.LineString:\n",
    "        routes_new.append([name, category, [[g[0], g[1]] for g in list(geo.coords)]])\n",
    "    else:\n",
    "        for ge in geo.geoms:\n",
    "            routes_new.append([name, category, [[g[0], g[1]] for g in list(ge.coords)]])\n",
    "\n",
    "routes_df = pd.DataFrame(routes_new, columns= ['name', 'category', 'geo'])\n",
    "\n",
    "routes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_df.to_csv(\"data/routes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Frequencies\n",
    "We compute a time series for every stop in our one week interval for every 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute time series for the Rejseplan schedule this includes the frequency of departures at every stop, every hour for a week.\n",
    "ts = []\n",
    "for feed in feeds:\n",
    "    a = feed.compute_stop_time_series(dates=dates, freq='60Min').transpose().reset_index(level=1)\n",
    "    ts.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that takes a list of lists and adds elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lists(lists):\n",
    "    res = np.array([0]*len(lists[0]))\n",
    "    for i in range(len(lists)):\n",
    "        l = np.array(lists[i])\n",
    "        l = np.nan_to_num(l)\n",
    "        # print(f\"adding res with {np.shape(res)} and list with {np.shape(l)}\")\n",
    "        res = np.add(res, l)\n",
    "        # print(f\"resulting shape: {np.shape(res)}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create our initial h3 map which maps the frequency of departures as a sum of all departures during an hour within our specified hexagons. This means the value of a hexagon can be the sum of departures from multiple stops. We furthermore create a queue of stops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "for idx, feed in enumerate(feeds):\n",
    "    tts = ts[idx]\n",
    "    result_df['stop_id'] = tts['stop_id'].values.tolist()\n",
    "    result_df['h3'] = result_df.apply(lambda row: stop_h3_maps[idx][row['stop_id']], axis=1)\n",
    "    result_df['frequency'] = tts.iloc[:,1:].values.tolist()\n",
    "\n",
    "# unique_h3s = set(stop_h3_maps[idx].values())\n",
    "unique_h3s = [item for sublist in stop_h3_maps for item in sublist.values()]\n",
    "h3_map = {}\n",
    "h3_queue = []\n",
    "for index in unique_h3s:\n",
    "    h3_queue.append(index)\n",
    "    res = result_df.loc[result_df['h3'] == index]\n",
    "    if res.shape[0] > 1:\n",
    "        res = add_lists(res['frequency'].values.tolist())\n",
    "        h3_map[index] = res.tolist()\n",
    "    elif res['frequency'].any():\n",
    "        h3_map[index] = np.nan_to_num(res['frequency'].values[0]).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying stop types\n",
    "\n",
    "For each stop we identify which type of stop it is as specified in the GTFS specifications. We similarly find that custom stop types have been added and map these to a relevant stop type. As thus we encode stop types as follows\n",
    "* busses (stop code 3), Midttrafik busses (stop code 700 and 715) $\\to$ 0\n",
    "* trams and ferries (stop code 0 and 4) $\\to$ 1\n",
    "* S-train and metro (stop code 109 and 1) $\\to$ 2\n",
    "* Trains (stop code 1) $\\to$ 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_type_map = {}\n",
    "\n",
    "\n",
    "for _, route in feeds[0].routes.iterrows():\n",
    "    route_type = route['route_type']\n",
    "    # s tog becomes metro\n",
    "    if route_type == 109 or route_type == 1:\n",
    "        route_type = 2\n",
    "    # busses in midtrafik\n",
    "    elif route_type == 700 or route_type == 715 or route_type == 3:\n",
    "        route_type = 0\n",
    "    elif route_type == 2:\n",
    "        route_type = 3\n",
    "    else:\n",
    "        route_type = 1\n",
    "\n",
    "    stops = gk.get_stops(feeds[0], route_ids=[route['route_id']])['stop_id']\n",
    "    for stop in stops:\n",
    "        h3_index = stop_h3_maps[0][stop]\n",
    "        try:\n",
    "            current = stop_type_map[h3_index]\n",
    "            if route_type > current:\n",
    "                stop_type_map[h3_index] = route_type\n",
    "        except KeyError:\n",
    "            stop_type_map[h3_index] = route_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stop_type_map.json\", \"w\") as outfile:\n",
    "    json.dump(stop_type_map, outfile)\n",
    "with open(\"data/stop_h3_map.json\", \"w\") as outfile:\n",
    "    json.dump(stop_h3_maps[0], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from osm_convert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script filters through the danish OSM data keeping only way elements containing the `highway` key and any of the following values\n",
    "* \"footway\"\n",
    "* \"path\"\n",
    "* \"pedestrian\"\n",
    "* \"cycleway\"\n",
    "* \"residential\"\n",
    "* \"unclassified\"\n",
    "* \"tertiary\"\n",
    "* \"secondary\"\n",
    "We chose the above keys because of the scope of our project which is determined to cover transit accesibility for all of Denmark. As such, while tertiary and secondary roads might not be accesible by foot because there is no pavement we include them to accomodate the fact that for rural areas these types of stops do exist. For definitions see https://wiki.openstreetmap.org/wiki/Key:highway  \n",
    "Finally it removes the key value pair `foot:no` see https://wiki.openstreetmap.org/wiki/Key:foot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `osm_convert.py` script is too memory intensive to execute through a jupyter notebook, you will have to run this separately. To run this you will also need download the danish OSM data from the following link https://download.geofabrik.de/europe/denmark.html and put it in the ``notebooks`` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/dkall.geojson') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Switching coordinates\n",
    "for geo in data['features']:\n",
    "    geo['geometry']['coordinates'] =  [[y,x] for x,y in geo['geometry']['coordinates']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to convert OSM ways to H3 hexagons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function that can project different types of geometries to and from WGS 84 and the UTM zone 32N. We do this to work with buffering in metres. If we buffer using WGS 84 we are more likely to produce distortions when buffering an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer, CRS\n",
    "\n",
    "\n",
    "crs_4326 = CRS.from_epsg(4326)\n",
    "crs_25831 = CRS.from_epsg(25832)\n",
    "\n",
    "to_dk = Transformer.from_crs(crs_4326, crs_25831)\n",
    "to_WGS = Transformer.from_crs(crs_25831, crs_4326)\n",
    "\n",
    "\n",
    "def CRS_transform(shp, inv=False):\n",
    "    geoInterface = shp.__geo_interface__\n",
    "\n",
    "    shpType = geoInterface['type']\n",
    "    coords = geoInterface['coordinates']\n",
    "    if shpType == 'Polygon':\n",
    "        if inv:\n",
    "            newCoord = [[to_WGS.transform(*point) for point in linring] for linring in coords]\n",
    "        else:\n",
    "            newCoord = [[to_dk.transform(*point) for point in linring] for linring in coords]\n",
    "    elif shpType == 'LineString':\n",
    "        if inv:\n",
    "            newCoord = [to_WGS.transform(*point) for point in coords]\n",
    "        else:\n",
    "            newCoord = [to_dk.transform(*point) for point in coords]\n",
    "    elif shpType == 'MultiPolygon':\n",
    "        if inv:\n",
    "            newCoord = [[[to_WGS.transform(*point) for point in linring] for linring in poly] for poly in coords]\n",
    "        else:\n",
    "            newCoord = [[[to_dk.transform(*point) for point in linring] for linring in poly] for poly in coords]\n",
    "\n",
    "    return shape({'type': shpType, 'coordinates': tuple(newCoord)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polyfill method\n",
    "\n",
    "The first method goes through every object and projects the coordinates to the proper CRS with metric units. We then buffer the objects using a 10 meter buffer and reproject back to WGS 84. Then we invoke the H3 polyfill function which transforms a polygon into h3 indices. This is slow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import mapping, shape\n",
    "\n",
    "\n",
    "# h3_set = set()\n",
    "# for geo in data['features']:\n",
    "#     tmp = shape(geo['geometry'])\n",
    "#     tmp = CRS_transform(tmp)\n",
    "#     tmp = tmp.buffer(10)\n",
    "#     tmp = CRS_transform(tmp, inv = True)\n",
    "\n",
    "#     if mapping(tmp)['type'] == 'Polygon':\n",
    "#         h3_set.update(h3.polyfill(mapping(tmp), 11, geo_json_conformant=False))\n",
    "#     elif mapping(tmp)['type'] == 'MultiPolygon':\n",
    "#         polygons = list(tmp)\n",
    "#         for poly in polygons:\n",
    "#             h3_set.update(h3.polyfill(mapping(poly), 11, geo_json_conformant=False))\n",
    "#     else:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line interpolation method\n",
    "\n",
    "For this method we reproject the gemetries from the WGS 84 CRS to the UTM zone 32N with metric units. We then create points along the way objects for every 10 meters using shapely interpolation. The point coordinates can then be used to make h3 indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import mapping, shape, LineString\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# h3_set = set()\n",
    "# for geo in data['features']:\n",
    "#     tmp = shape(geo['geometry'])\n",
    "#     tmp = CRS_transform(tmp)\n",
    "#     distances = np.arange(0, tmp.length, 10)\n",
    "#     points = [tmp.interpolate(distance) for distance in distances] \n",
    "#     if len(points) == 1:\n",
    "#         continue\n",
    "#     tmp = LineString(points)\n",
    "#     tmp = CRS_transform(tmp, inv = True)\n",
    "#     h3s = {h3.geo_to_h3(lat, lon, 11) for (lat, lon) in mapping(tmp)['coordinates']}\n",
    "#     h3_set.update(h3s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3 line method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we ended up using was to use the h3 line function which returns the indices of the hexagons forming a line between two points. We could therefore iterate over the points in every way object from OSM data and generate our walkable hexagons that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h3_set = set()\n",
    "for geo in data['features']:\n",
    "    tmp = shape(geo['geometry'])\n",
    "    if len(mapping(tmp)['coordinates']) == 1:\n",
    "        h3_set.add(h3.geo_to_h3(*mapping(tmp)['coordinates'], resolution = 11))\n",
    "    for i in range(len(mapping(tmp)['coordinates']) - 1):\n",
    "        h3s = h3.h3_line( h3.geo_to_h3(*mapping(tmp)['coordinates'][i], resolution = 11) , h3.geo_to_h3(*mapping(tmp)['coordinates'][i+1], resolution = 11) )\n",
    "        h3_set.update(h3s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk = pd.DataFrame(list(h3_set), columns=['h3'])\n",
    "walk.to_csv(\"data/line_paths_11.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3 edges method\n",
    "\n",
    "This method is similar to the h3 line function described above but only returns the unidirectional edges of the hexagons making up our paths. It could however be used to model flows in future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_set = set()\n",
    "for geo in data['features']:\n",
    "    tmp = shape(geo['geometry'])\n",
    "    if len(mapping(tmp)['coordinates']) == 1:\n",
    "        continue\n",
    "        h3_set.add(h3.geo_to_h3(*mapping(tmp)['coordinates'], resolution = 11))\n",
    "    for i in range(len(mapping(tmp)['coordinates']) - 1):\n",
    "        h3s = h3.h3_line( h3.geo_to_h3(*mapping(tmp)['coordinates'][i], resolution = 11) , h3.geo_to_h3(*mapping(tmp)['coordinates'][i+1], resolution = 11) )\n",
    "        h3_edges = [h3.get_h3_unidirectional_edge(h3s[i], h3s[i+1]) for i in range(len(h3s) -1)]\n",
    "        h3_set.update(h3_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walk = pd.DataFrame(list(h3_set), columns=['h3'])\n",
    "#walk.to_csv(\"data/edges_paths_11.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add walking network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk = pd.read_csv(\"data/line_paths_11.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check whether the walkable h3 indices are part of the h3 stop indices and assign them an array of zero frequencies if they are not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_map = {}\n",
    "\n",
    "for w in walk['h3']:\n",
    "    try:\n",
    "        h3_map[w]\n",
    "        continue\n",
    "    except KeyError:\n",
    "        walk_map[w] = [0]*(24*7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the walking network to the stops data\n",
    "\n",
    "Starting from our stops we go through neighboring hexagons and check whether they are part of the walkable h3 indices. If so, we add them and their frequencies to our h3 map data we then add them to queue and keep track of which hexagons we have visited. By doing this we add our walking network as well as dropping indices that are not connected to a stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_map = {}\n",
    "\n",
    "while h3_queue:\n",
    "    current = h3_queue.pop()\n",
    "    visited_map[current] = True\n",
    "    for hexagon in h3.k_ring(current, 1):\n",
    "        # add hexagon to the network\n",
    "        try:\n",
    "            h3_map[hexagon] = walk_map[hexagon]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        # add unvisited hexagons to the queue\n",
    "        try:\n",
    "            visited_map[hexagon]\n",
    "            continue\n",
    "        except KeyError:\n",
    "            h3_queue.append(hexagon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save a checkpoint in case we run into ram issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/h3_map.json\", \"w\") as outfile:\n",
    "    json.dump(h3_map, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be wise to do free memory as you will be needing at least 16 GB later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading EU urbanization data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with raster image data and utilize the rasterio library to handle the layered nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from rasterio.plot import show\n",
    "import rasterio.warp\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.enums import Resampling\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import shapely as shp\n",
    "import gtfs_kit as gk\n",
    "from shapely.geometry import mapping, shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data visualized. Every pixel value corresponds to a label of a $1km^2$ square. The data is documented here https://ghsl.jrc.ec.europa.eu/documents/GHSL_Data_Package_2019.pdf pages 17-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'data/GHS_SMOD_POP2015_GLOBE_R2019A_54009_1K_V2_0_18_2.tif'\n",
    "src = rasterio.open(fp)\n",
    "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "\n",
    "\n",
    "show(src, ax =ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above picture clearly omits Bornholm. The reason is that the data is downloaded on a per grid level. We therefore need to include the grid containing Bornholm.\n",
    "\n",
    "In the following code we go through both grids containing the danish data. We start by upsampling the resolution of the images to a 10 times higher resolution and interpolate image values based on bilinear interpolation. This is done to avoid geographical inconsistencies when assigning values to H3 hexagons across larger differences in resolution. We construct arrays of the the coordinate values of the upsampled image pixels, as specified by the CRS of the data which is \"World_Mollweide\". We transform the coordinates to WGS 84, create the h3 indices at resolution 9 otherwise our map will be incomplete. We specify urban areas as pixels with values 21, 22, 23 or 30 and values 11, 12, 13 as rural areas as specified in the data documentation. Pixels with value 10 are water pixels and they are removed. Finally, we assign hexagons at resolution 11 the corresponding urban-rural label depending on their parent h3 index. If a parent h3 index is from a water pixel we assign it as a rural hexagon.  \n",
    "\n",
    "`Requires quite a good amount of ram (16GB) and time` If you run into RAM issues, data should be importable through data checkpoints made in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = ['data/GHS_SMOD_POP2015_GLOBE_R2019A_54009_1K_V2_0_18_2.tif','data/GHS_SMOD_POP2015_GLOBE_R2019A_54009_1K_V2_0_19_2.tif']\n",
    "urban_map = dict()\n",
    "for i, fp in enumerate(fps):\n",
    "    \n",
    "    with rasterio.open(fp) as src:\n",
    "        b = src.read(\n",
    "        out_shape=(\n",
    "            src.count,\n",
    "            int(src.height * 10),\n",
    "            int(src.width * 10)\n",
    "        ), resampling=Resampling.bilinear)\n",
    "\n",
    "        transform = src.transform * src.transform.scale(\n",
    "                src.height / b.shape[-1],\n",
    "                src.width / b.shape[-2]\n",
    "            )\n",
    "\n",
    "        print('Band{i} has shape', b.shape)\n",
    "        b, trans = rasterio.warp.reproject(\n",
    "            source = b,\n",
    "            src_transform= transform,\n",
    "            src_crs = CRS.from_wkt('PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],\\\n",
    "                AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],\\\n",
    "                PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'),\n",
    "            dst_crs = CRS.from_epsg(4326)\n",
    "        )\n",
    "\n",
    "        height = b.shape[-2]\n",
    "        width = b.shape[-1]\n",
    "        ys, xs = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        xs, ys = rasterio.transform.xy(trans, xs, ys)\n",
    "        xs= np.array(xs).flatten()\n",
    "        ys = np.array(ys).flatten()\n",
    "        for x,y, val in zip(ys, xs, b.flatten()):\n",
    "            if val == 10 or val == 0:\n",
    "                continue\n",
    "            elif val == 21 or val == 22 or val == 23 or val == 30: #urban pixel values\n",
    "                urban_map[h3.geo_to_h3(x, y, resolution=9)] = 1\n",
    "            else:\n",
    "                urban_map[h3.geo_to_h3(x,y, resolution=9)] = 0\n",
    "with open(f\"data/urban.json\", \"w\") as outfile:\n",
    "    json.dump(urban_map, outfile)\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data as neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/h3_map.json') as json_file:\n",
    "    h3_map = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/urban.json') as json_file:\n",
    "    urban_map = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hexagon in h3_map.keys():\n",
    "    par = h3.h3_to_parent(hexagon, 9)\n",
    "    try: # some parents are water\n",
    "        h3_map[hexagon] = (h3_map[hexagon], urban_map[par]) \n",
    "    except:\n",
    "        h3_map[hexagon] = (h3_map[hexagon], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/h3_map.json\", \"w\") as outfile:\n",
    "    json.dump(h3_map, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data if neccesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_type_map.json') as json_file:\n",
    "    stop_type_map = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/h3_map.json') as json_file:\n",
    "    h3_map = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataset creation\n",
    "\n",
    "Here we put together the final version of our raw dataset containing h3 indices at resolution 11 for computation, at resolution 10 for aggregation and visualization, and at resolution 4 for fast rendering in visualization. The every index at resolution 11 has an array of length 24*7 containing the frequency of departures at every hour of the week at stops, while for paths all values are 0. The indices also have information about the whether it is urban or rural and what type of stop the index is, as specified by our stop map. For non-stops this value is -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame()\n",
    "new['h3'] = h3_map.keys()\n",
    "new['h3_group'] = new.apply(lambda row: h3.h3_to_parent(h = row['h3'], res=4), axis=1)\n",
    "new['h3_group_agg'] = new.apply(lambda row: h3.h3_to_parent(h = row['h3'], res=10), axis=1)\n",
    "new['freq'] = [x[0] for x in h3_map.values()]\n",
    "new['urban'] = [x[1] for x in h3_map.values()]\n",
    "new['type'] = new.apply(lambda row: -1 if sum(row['freq']) == 0 else stop_type_map[row['h3']], axis=1)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_json(\"data/dataframe.json\", orient = 'records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis of spatial container sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did a bit of data analysis on the size of danish spatial containers to determine an appropriate scaling factor of avaiability between rural and urban areas. The spatial containers represent a conceptualization of the hierarchical nature of human cognitive maps. The methodology of computing these spatial containers as well as an introduction to the data can be found here: \"The scales of human mobility\" (Alessandreti, et. al, 2020) https://www.nature.com/articles/s41586-020-2909-1 \n",
    "\n",
    "The data used here is derived data and can be found at https://data.dtu.dk/articles/dataset/The_Scales_of_Human_Mobility/12941993/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data and extract the sizes of the level 2 container sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtu = pd.read_pickle('data/Figure_3.pkl')\n",
    "dtu = dtu[dtu['NAME_0']=='Denmark']\n",
    "dtu = dtu[dtu.sizes.apply(lambda x: len(x) != 0)]\n",
    "\n",
    "\n",
    "l_2 = []\n",
    "for sizes in dtu['sizes']:\n",
    "    l_2.append(sizes[0])\n",
    "dtu['l_2'] = l_2\n",
    "\n",
    "dtu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has very long tails (see below) so we either need to handle some of the extreme cases or look at the data on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joypy\n",
    "\n",
    "fig, axes = joypy.joyplot(dtu, by=\"n_scales\", column=\"l_2\", figsize=(8,6),\n",
    "                          title=\"Container sizes\", kind = 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a couple of function to remove \"outliers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(s):\n",
    "    lower_limit = s.mean() - (s.std() * 2)\n",
    "    upper_limit = s.mean() + (s.std() * 2)\n",
    "    return ~s.between(lower_limit, upper_limit)\n",
    "\n",
    "def is_windsor_outlier(s):\n",
    "    lower_limit, upper_limit = s.quantile([0.05, 0.5])\n",
    "    return ~s.le(upper_limit) \n",
    "\n",
    "def iqr_50(s):\n",
    "    lower_limit, upper_limit = s.quantile([0.25, 0.75])\n",
    "    return ~s.between(lower_limit, upper_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics for urban data, we group by the number of scales each individual hasas this influences their container sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban = dtu[(dtu.urban_rural == 'urban')] # (~dtu.n_scales.isin([2,3])\n",
    "urban['log_l_2'] = urban['l_2'].apply(lambda x: np.log(x))\n",
    "urban_f = urban[~urban.groupby('n_scales')['l_2'].apply(is_windsor_outlier)]\n",
    "urban.groupby('n_scales').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = joypy.joyplot(urban, by=\"n_scales\", column=\"log_l_2\", figsize=(8,6),\n",
    "                          title=\"Urban container sizes, log scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that even though these container sizes should represent an area in which each location within the area is considered close. However the mean of each container is more than 2 km with an even larger standard deviation. On the log scale the data might make a bit more sense, however it is immediately apparent that we cannot use container sizes as a measure of what is a good distance to public transport especially when considering accesibility by foot.\n",
    "\n",
    "What we might try to do is to see if we can determine a scaling factor between the rural and urban sizes which we can use for our accesibility calculations. First we try fitting a Z-score standardizer to the urban container sizes and then transform the rural scales using the urban standardizer. This might give us a better sense of much larger rural containers are than urban. One limitation though is that the rural population has more levels than the urban though this only affects 3 individuals here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_scales = {}\n",
    "for i, scale in urban.groupby(\"n_scales\"):\n",
    "    urban_scales[i] = StandardScaler().fit(X = scale[['log_l_2','l_2']])\n",
    "#urban[['log_l_2_s','l_2_s']] = urban_scale.transform(urban[['log_l_2','l_2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rural = dtu[(dtu.urban_rural == 'rural')] # & (~dtu.n_scales.isin([2,3]))\n",
    "rural['log_l_2'] = rural['l_2'].apply(lambda x: np.log(x))\n",
    "rural_f = rural[~rural.groupby('n_scales')['l_2'].apply(is_windsor_outlier)]\n",
    "rural.groupby('n_scales').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = joypy.joyplot(rural, by=\"n_scales\", column=\"log_l_2\", figsize=(8,6),\n",
    "                          title=\"Rural container sizes, log scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the weighted average rural container size expressed as Z-scores from the scaler fitted on urban data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rural_resc = pd.DataFrame()\n",
    "for i, scale in rural.groupby('n_scales'):\n",
    "    try:\n",
    "        scale[['log_l_2_s','l_2_s']] = urban_scales[i].transform(scale[['log_l_2','l_2']])\n",
    "        rural_resc = rural_resc.append(scale)\n",
    "    except:\n",
    "        rural_resc = rural_resc.append(scale)\n",
    "\n",
    "\n",
    "\n",
    "rural_res = rural_resc[['log_l_2_s','l_2_s', 'n_scales']].groupby('n_scales').describe()\n",
    "rural_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rural_res.reset_index()['log_l_2_s']['mean'] * rural_res.reset_index()['log_l_2_s']['count'] / rural_res.reset_index()['log_l_2_s']['count'].sum()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the size in rural areas on average is 0.5 standard deviation larger than the mean of cities. However since the we are working with logarithms it is difficult to translate this into actual distances that we can use for scaling since it will depend the initial mean value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try windzorizing the non-transformed sizes to arrive at at scale in meters that makes sense to consider in relation to transit. We then try to fit mixture model of two gaussians to both rural and urban data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the urban data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gmm = GaussianMixture(\n",
    "            n_components=2, covariance_type='full', random_state=0\n",
    "        )\n",
    "gmm.fit(np.expand_dims(urban_f['l_2'], 1))\n",
    "Gaussian_nr = 1\n",
    "print(f\"Results for all scales\")\n",
    "for mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n",
    "    print(f'Normal_distb {Gaussian_nr}: μ = {mu}, σ = {sd}, weight = {p}')\n",
    "    Gaussian_nr += 1 \n",
    "\n",
    "for scale, scales_group in urban_f.groupby('n_scales'):\n",
    "    gmm = GaussianMixture(\n",
    "            n_components=2, covariance_type='full', random_state=0\n",
    "        )\n",
    "    gmm.fit(np.expand_dims(scales_group['l_2'], 1))\n",
    "    Gaussian_nr = 1\n",
    "    print(f\"Results for {scale} scales\")\n",
    "    for mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n",
    "        print(f'Normal_distb {Gaussian_nr}: μ = {mu}, σ = {sd}, weight = {p}')\n",
    "        Gaussian_nr += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(\n",
    "            n_components=2, covariance_type='full', random_state=0\n",
    "        )\n",
    "gmm.fit(np.expand_dims(rural_f['l_2'], 1))\n",
    "Gaussian_nr = 1\n",
    "print(f\"Results for all scales\")\n",
    "for mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n",
    "    print(f'Normal_distb {Gaussian_nr}: μ = {mu}, σ = {sd}, weight = {p}')\n",
    "    Gaussian_nr += 1 \n",
    "\n",
    "for scale, scales_group in rural_f.groupby('n_scales'):\n",
    "    gmm = GaussianMixture(\n",
    "            n_components=2, covariance_type='full', random_state=0\n",
    "        )\n",
    "    gmm.fit(np.expand_dims(scales_group['l_2'], 1))\n",
    "    Gaussian_nr = 1\n",
    "    print(f\"Results for {scale} scales\")\n",
    "    for mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n",
    "        print(f'Normal_distb {Gaussian_nr}: μ = {mu}, σ = {sd}, weight = {p}')\n",
    "        Gaussian_nr += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end taking inspiration from the work here when determining the scaling factor for the urban rural distinction but don't choose a specific empirically derived scale."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f29657b77c8bf829777364c81e5ce81492492df16c90ee8ecf1bc70e3f888ba"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('transit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
